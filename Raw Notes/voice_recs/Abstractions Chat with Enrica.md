Dicevo, sostanzialmente l'idea su cui non mi caga nessuno è il discorso di abstractions, cioè soprattutto nell'ottica del grant di Leonardo. Il mio problema principale, ora come ora, è mantenere ordine nello spazio dei task e delle astrazioni su cui si lavora. Che è il problema principale, ma in generale con le robe di linguaggio. Perché? Perché è sempre questa idea surreale che abbiamo, che è achievable avere dei modellini ultra-semplificatori di tutto. Quindi tu puoi avere qualunque cosa, cioè vogliamo che il language model faccia tutto, però poi dall'altra parte ci aspettiamo che sia super, che sia astraibile quello che sta facendo in maniera super semplicistica. E quindi in realtà, cosa che non è vera, per cui noi abbiamo tutta una serie di... Abbiamo questa situazione principale in cui di fatto il language model crea queste arbitrarie mappe tra stringhe e stringhe. E qualunque stringhe è possibile. Però abbiamo anche aggiunto questo altro secondo concetto che le stringhe sono parsabili in delle entità, delle categorie e delle relazioni fra di loro. E l'idea che la mappa tra questo parsing tra stringhe e categorie deve in qualche maniera seguire una struttura e l'utilizzo delle JSON string è, data quanto sono presenti in giro e dato che sono l'oggetto di base che viene utilizzato per trasferire API calls e mappare API calls da stringhe a oggetti strutturati, sono il candidato ideale. Piedantic si dimostra... Piedantic dimostra un pacchetto che permette di mappare il concetto che magari è più simile a quello che pensiamo ora di linguaggio, di classi, entità e strutture. C'è da notare che questo è tutto concetto di teoria delle categorie di cui non sapevo un cazzo, obiettivamente. E permette di mappare quindi automaticamente queste strutture, astrazioni, piedantic models in JSON string. e quindi conseguentemente in stringa 

ldetto ciò, il punto che ora volevo avere è che dobbiamo iniziare a costruire oggettivamente una tassonomia di queste paid-ante classes e io qui ho un conflitto che mi diventa un po' difficile per me penso che la maniera giusta di farle sia provare a fare dei task, creare le paid-ante classes che servono per fare quel task e mano a mano capire quali ci sono in comune fra più task e poi farle diventare, sostanzialmente generalizzare, astrarre d'altra parte abbiamo anche capito che se uno si mette in maniera ossessiva creiamo delle strutture molto nested e quando uno lavora in una struttura nested non è oggettivamente necessario e forse non c'è vantaggio di lavorare in una struttura nested ma è un'ottima cosa per la nostra vita quindi una struttura gerarchica che applica un'operazione di coarse graining rispetto ai livelli più bassi della gerarchia se noi stiamo guardando un libro e lo guardiamo in considerazione di capitolo stiamo facendo una qualche operazione di aggregazione non lineare delle frasi che è in sé un'aggregazione non lineare delle parole che è in sé un'aggregazione non lineare dei token questo tipo di operazioni nested nel momento in cui uno inizia ad essere ossessivo e provare a descrivere tutto è la prima cosa che ci è venuta fuori però abbiamo visto in pratica che non è così buono perché richiede l'utilizzo di un sacco di token inutili per fare queste cose e l'idea di avere queste astrazioni gerarchiche è che nel momento in cui tu ti stai occupando di un paragrafo hai bisogno soltanto di sapere quali sono le frasi che sono contenute dentro e qual è la loro rappresentazione e quali sono le loro parole che sono contenute dentro dentro e quindi non è necessario andare più basso di livello perché altrimenti non avrebbe senso il ragionamento in termini di core screening quindi una parte fondamentale che dobbiamo avere di queste astrazioni è da una parte una cosa che mantenga questa struttura gerarchica dall'altra la capacità di utilizzare queste astrazioni come se fossero più shallow o soltanto di fare uno zoom specifico di quali sono i livelli semantici a cui ci stiamo preoccupando quindi. intendo shallow, nel senso quando intendo shallow intendo la struttura del json che deriva dal modello quanti livelli di nesting ha, quindi ribadisco è possibile vedere un capitolo come una collezione di paragrafi che sono una collezione di frasi, che sono una collezione di parole, che sono una collezione di tokens oppure vedere un capitolo come una collezione di frasi e basta, come una collezione di paragrafi e basta quindi l'idea sostanziale qui è che si può fare una decisione di quanto raffinata è la risoluzione di un'analisi e a che livello di coarse graining, o fine graining dal contrario, stiamo considerando le cose quindi c'è un livello di zoom semantico che chiamerei qui e questo livello di zoom semantico sostanzialmente tanto ci permette di decomporre task quindi prima estraiamo magari dei paragrafi e le frasi se poi vogliamo lavorare sulle frasi a un livello più basso lo guardiamo a livello di frasi e parole o frasi e parole e tokens quindi possiamo in qualche maniera definire le risoluzioni a cui devono essere tenute in considerazione e la struttura a livello di frase e parole. Ok, a questo punto in generale possiamo anche pensare che i nostri dati sono di fatto in sé sempre, hanno una rappresentazione in termini di paidantic classes anche prima di essere trasformati. Chiarimento, per ora abbiamo parlato di language model come modelli tra stringhe e stringhe e abbiamo, utilizzando la generazione strutturata, esteso questo pensiero a language model come mappe tra stringhe e categorie. Nel momento in cui abbiamo raffinato il nostro pensiero, abbiamo iniziato a pensare al concetto di coarse graining e livelli di astrazione, abbiamo visto che è facile iniziare a pensare istantaneamente ai language model come mappe tra categorie e categorie. Ad esempio, un language model prende come input una categoria chiamata paragrafo e outputa dalla sua mappa, dalla sua trasformazione strutturata, una categoria chiamata paragrafo, le cui frasi sono labellizzate a livello emotivo, sintatico e grammaticale.  In questi pensieri adesso aggiungiamo l'idea che queste categorie non siano soltanto valide per oggetti già processati dai language model, ma anche per oggetti già processati dai language model. Possiamo definire categorie rispetto ai nostri dati. Nella situazione principale abbiamo una categoria di un dato testuale con una provenienza sconosciuta.  Quindi i nostri dati che arrivano potrebbero avere una paidantic classes per definire dei dati grezzi di cui non sappiamo la provenienza, potremmo avere appunto del raffinamento di dato grezzo che può essere un processed book o core del genere. E possiamo quindi in una qualche maniera vedere questa applicazione di structured extraction con language model come la creazione di delle mappe. Tra categorie. Quindi se un dato inizialmente grezzo può essere processato in un raw book, che potrebbe essere un book di cui non sappiamo la separazione in capitoli, di cui sono presenti all'interno dei dati, tutta una serie di tag XML o artefatti del OCR scanning del libro, o misspacing eccetera, quindi pensiamo potrebbe essere un, potremmo chiamare appunto un broken markdown o qualcosa del genere. E questa è una categoria e l'output che è processed book invece crea una mappa fatta o da mano da una persona o attraverso l'applicazione di sofisticate sequenze di regular expression o attraverso un language model, crea una mappa che lo porta a una nuova classe, il processed book, che poi ora può essere successivamente, che è libero di artefatti e ha la garanzia di avere una struttura per cui una serie di algoritmi per la separazione del libro in capitoli, paragrafi, frasi può essere applicata. Ok. Quindi per sommarizzare, l'idea è che Abstraction permette di costruire un mapping tra spazi categorici e quindi l'idea di Abstraction sarebbe di essere una collezione di astrazioni, quindi definisce il supporto delle maniere in cui il testo può essere mappato all'interno di fatto di un cervello umano e le maniere in cui queste astrazioni si mappano l'una nell'altra. Quindi abbiamo due aspetti, le astrazioni e le trasformazioni tra astrazioni. Grazie. L'ultima separazione che abbiamo sono la separazione tra le astrazioni che riguardano direttamente l'artefatto e le astrazioni che riguardano il mondo contenuto o descritto all'interno dell'artefatto. Quindi noi potremmo avere delle astrazioni, ad esempio libro, che descrivono dei dati, quindi i nostri dati sono un libro, ma nei dati stessi si potrebbe parlare di libri, si potrebbe parlare di lettura o scrittura, o un libro all'interno della storia stessa può ricevere delle trasformazioni. Nella stessa maniera un libro intero. In sé può essere trasformato perché rivisto, editato da un editor o addirittura censurato da una dittatura. In questa conseguenza quindi abbiamo bisogno di separare non solo le astrazioni, ma a che realtà appartengono. 

Non completamente legato in termini cronologici rispetto al discorso, ma pensiero interessante che è venuto è l'idea che sostanzialmente quello che i padantic models sono, sono una maniera, un operatore per definire classi di equivalenza rispetto alle stringhe. Quindi classi di equivalenza che dalla nostra interpretazione derivano dal mapping in uno spazio di entità semantico, di estrazioni, di categorie come le vogliamo chiamare, ma di fatto creano una classe di equivalenza tra stringhe che vengono rifiutate e stringhe che vengono accettate, cioè validate dal padantic class. 


L'altra cosa che secondo me rimane interessante, per cui questo concetto mi stuzzica molto, è tutta la fatica fatta nell'ultimo anno nel design del videogioco, nel provare a capire le astrazioni necessarie e anche la parte di codice per fare un game engine. Che sia espandibile e che sia scrivibile che si è scrivibile da language models. Appunto, lì, alla fine, le astrazioni erano state raggruppate all'interno di queste categorie. Entities, che sono popolate da attributes. Statements, che sono espressioni logiche rispetto agli attributes. Propositions, che sono dichiarazioni di valori rispetto agli statements di una o più entità in un particolare momento del tempo. Affordances, che descrivono i requisiti in termini di attributes e di statements che devono essere validi per quegli attributes affinché delle trasformazioni possano avvenire su delle entità e le conseguenti attributes, quindi le proposizioni conseguenti dell'applicazione di quella trasformazione. Poi ci sono le azioni che sono il grounding di una foredance in due gruppi di source e target entities e ovviamente tutti i requisiti della foredance per tutte le entità partecipanti devono essere soddisfatte. Grazie. L'estensione temporale delle azioni è il concetto di opzione temporale, derivante dal reinforcement learning e dalla teoria del planning, che permette di creare catene di azioni. I prerequisiti e le conseguenze logiche di un'opzione possono essere calcolate utilizzando algoritmi di propagazione forward o backward logica. La composizione di queste astrazioni e dei processi di ricerca all'interno dello spazio delle opzioni definiscono il core del Goal Oriented Action Planning, che è un framework di pianificazione sviluppato all'MIT durante gli anni 2000, che sta poi alla base dell'AI di un sacco di videogiochi.

Per quanto questo aspetto sia interessante, sono un po' preoccupato dalla sofisticazione e dalle complessità logiche che ho trovato nelle mie precedenti implementazioni di Goal Oriented Action Planning, anche detto GoUp. E quindi penso che mentre da una parte mantenere una separazione e mantenere una bank, un dizionario delle astrazioni utilizzate, delle entità nella creazione di queste astrazioni, penso che sia anche pericoloso forzare il GoUp framework automaticamente sulla creazione di queste PID. Penso che la cosa che... La cosa che più mi preoccupa al momento sia la relazione tra le astrazioni, degli artefatti effettivi, quindi dei dati, delle frasi, delle parole che ci sono dentro e degli artefatti, diciamo, impliciti che vengono evocati dal testo all'interno di un altro essere umano o un altro world model in grado di decodificarne il segnale. 


Ok, please, now translate the text in English and make a summary of what has been said. Be very exhaustive, don't forget anything, please, and avoid removing too much stuff.  

