Mixtral Hermes translation of the voice-note --> [[Ideas about learning classifiers of libcst nodes]]
The idea I want to discuss is essentially a project for a first paper on studying the ability of language models in general and more specifically of embedding models, to understand Python code. Specifically, the idea is to use [[LibCST]] to create concrete syntax trees of the code and utilize them to create labels of the types of nodes used within a snippet of Python code. The goal of the study is to see if we can create predictive models that, given embeddings, predict the syntactic node labels. So, reiterating, the idea is that we can use the concrete syntax tree to create discrete representations of the syntactic content of Python code and learn a predictive model that predicts the syntactic label using the code representation as a string, embedded by a language model, and this should be a measure of the ability of the language model to represent and understand the code in functional terms.

So, the first step is to create a dataset collection, consisting of a collection of Python code repositories, and parse the Python code using LibCST into classes and methods. And load it in a Polars Frame. For this part we can reuse a lot of the code that we had in BabyDragon for code frames. We most likely should create a new package just for that. Then, it would probably be best to separate dataset in two representations, the representation as classes and the representation as methods, which will generally have very different lengths and distributions of syntactic nodes. All the experiments we will talk about later in the maximum line will be applied to classes and functions as two different levels of representation, hence duplicating the experiments.

If we want to use multiple closed-source models via API, we cannot afford a dataset that is too large, and staying between one million and 10 million tokens would be ideal. This may be somewhat restrictive in terms of dataset size, so we need to see the quantities for a moment. In general, 100 million tokens are roughly $2 per model for small and $12 for large. Using a larger size would mean spending up to potentially hundreds of euros with the largest embeddings for each comparison. We think about doing 4, 5, 6, 7 comparisons, so we could be in the range of thousands of euros if we manage to stay below â‚¬100 for experiments would be better. Of course it would definitely be good to have a billion tokens dataset and release a billion Python tokens embedded with all the various embedders.

Since we want to use some models from [[HuggingFace]] and, among those really open source, the process could take quite a while, especially if we want to reach a billion. So first of all, we need to understand how to maximize the use of local GPUs and possibly figure out, probably using Modal, how to scale the process. However, this is very good because it would also be part of a contribution that we should make to Cynde.

The second step instead, after creating these datasets, will be to take a look at the distribution of nodes, see if they are interesting or not. I think we'll probably want to try classifying the dataset using embeddings and possibly use alternative features for the same task, such as syntax tree node histograms instead of the string representation of the code. And I also think that we will probably want to use the embedding of the string version of the concrete syntax tree of the snippet.

So, for each code snippet, we have multiple representations. The first is the textual representation of the code as a string defining the code snippet. The second is the string representation of the syntactic tree. It's the equivalent to that snippet of code as output from libcst. And third is the representation in terms of node histograms of the syntactic tree. So, this last one is a categorical representation or it could be thought of as a vector since there are multiple categories. The first two string representations can be transformed into vectors using language models or embedding models.

Finally, instead, in the third step, which turns out to be the conclusive analysis of the paper, I would start from the ability of various embedding models to classify, let's say, the possibility of learning predictive models from different embedding models that allow mapping from snippets to the categorical space of the syntactic tree. The conclusion here would be to create a ranking between embedders in terms of their ability within the vector mapping, typically considered semantic, to retain memory of embedded code syntax

From the point of view of the evaluation of the performance of the models we will follow the best practices defined in Cynde where we will focus on the use of the matthew correlation coefficient because the labels of the synthetic nodes will be very unbalanced. We will use nested stratified cross-validation, stratifying across syntax labels and package/repository of origin of the code snippet. Finally we will also do experiments in the ability of the model to generalize from one package to another using a cross-validation strategy with purging of the Python packages. 

Considering update dates of packages or datasets is essential to ensure fair evaluation. Ideally, obtaining the last modification date for each file within them can help control for potential direct training overlap between embedding models and evaluation data, minimizing any risk of testing a model whose embeddings have been trained on test data.